{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TP5: Training Tricks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset and define a transformation done on all images:\n",
        "# - resize to 32x32 (does nothing here)\n",
        "# - convert to tensor\n",
        "# - normalize to mean 0.5, std 0.5 for each channel\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "trainset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "testset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "print(f\"Train points {len(trainset)}\")\n",
        "print(f\"Test points {len(testset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show a few images from the training loader\n",
        "train_dl = DataLoader(trainset, batch_size=8, shuffle=True)\n",
        "imgs, labels = next(iter(train_dl))\n",
        "imgs = imgs[:8] * 0.5 + 0.5\n",
        "grid = make_grid(imgs, nrow=4)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis(\"off\")\n",
        "plt.title(\", \".join([trainset.classes[int(ell)] for ell in labels[:8]]))\n",
        "plt.show()\n",
        "\n",
        "print(f\"Number of classes: {len(trainset.classes)}\")\n",
        "# print(f\"Classes: {trainset.classes}\")\n",
        "print(f\"Image shape: {imgs[0].shape}\")  # C, H, W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Our Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our baseline model\n",
        "class CNN_base(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        layers = []\n",
        "        for i in range(5):\n",
        "            layers += [\n",
        "                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "            ]\n",
        "            if i in {1, 3, 4}:  # downsample after 2nd, 4th, and 5th conv\n",
        "                layers.append(nn.MaxPool2d(2, 2))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n_train = 200\n",
        "train_subset = Subset(trainset, list(range(n_train)))\n",
        "train_dl = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(testset, batch_size=4)\n",
        "\n",
        "writer = SummaryWriter(\"runs/\")\n",
        "\n",
        "# log a small batch of images and the model graph (if possible)\n",
        "imgs_sample, labels_sample = next(iter(train_dl))\n",
        "imgs_sample = imgs_sample.to(device)\n",
        "# writer.add_graph(model, imgs_sample)  # may fail for some models\n",
        "grid = make_grid(imgs_sample[:16], nrow=4, normalize=True, scale_each=True)\n",
        "writer.add_image(\"train/sample_images\", grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Our Training Set \n",
        "We focus on a tiny subset of CIFAR to test optimization on a simplet setup.\n",
        "Normally, we should end up overfitting this small training set quickly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = 200\n",
        "train_subset = Subset(trainset, list(range(n_train)))\n",
        "train_dl = DataLoader(train_subset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some utility functions to log info during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_grad_norms(model, writer, step, model_name=\"\"):\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is not None:\n",
        "            writer.add_scalar(f\"{model_name}/grads/{name}\", p.grad.norm().item(), step)\n",
        "\n",
        "\n",
        "def get_layer_grad_norms(model):\n",
        "    layer_norms = defaultdict(float)\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        layer_norms[name] += p.grad.norm().item() ** 2\n",
        "    return {k: v**0.5 for k, v in layer_norms.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, test_dl, criterion, writer=None, global_step=None, name=\"\"):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_dl:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = model(imgs)\n",
        "\n",
        "            losses.append(criterion(logits, labels).mean().numpy())\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    if writer is not None and global_step is not None:\n",
        "        writer.add_scalar(f\"{name}/test/accuracy\", 100 * correct / total, global_step)\n",
        "        writer.add_scalar(f\"{name}/test/loss\", np.mean(losses), global_step)\n",
        "    print(f\"Test accuracy: {100 * correct / total:.2f}%, test loss: {np.mean(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_dl,\n",
        "    opt,\n",
        "    criterion,\n",
        "    writer,\n",
        "    n_epochs,\n",
        "    name=\"\",\n",
        "    global_step=0,\n",
        "    prints=False,\n",
        "):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for imgs, labels in train_dl:\n",
        "            global_step += 1\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            writer.add_scalar(\"train/cnn_loss\", loss.item(), global_step)\n",
        "            if global_step % 10 == 0:\n",
        "                log_grad_norms(model, writer, global_step, model_name=name)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        print(f\"Epoch {epoch + 1}: train loss = {loss.item():.4f}\")\n",
        "        if prints:\n",
        "            print(get_layer_grad_norms(model))\n",
        "        model.eval()\n",
        "        test_model(\n",
        "            model, test_dl, criterion, writer=writer, global_step=global_step, name=name\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try sigmoid\n",
        "class CNN5_sigmoid(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        layers = []\n",
        "        for i in range(5):\n",
        "            layers += [\n",
        "                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),\n",
        "                nn.Sigmoid(),\n",
        "            ]\n",
        "            if i in {1, 3, 4}:  # downsample after 2nd, 4th, and 5th conv\n",
        "                layers.append(nn.MaxPool2d(2, 2))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = 200\n",
        "train_subset = Subset(trainset, range(n_train))\n",
        "train_dl = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "cnn_sig = CNN5_sigmoid().to(device)\n",
        "total_params = sum(p.numel() for p in cnn_sig.parameters())\n",
        "print(f\"Training SimpleCNN model with {total_params} parameters\")\n",
        "\n",
        "opt = torch.optim.Adam(cnn_sig.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "name = \"sig_act\"\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn_sig, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare to initial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"baseline\"\n",
        "cnn = CNN_base().to(device)\n",
        "total_params = sum(p.numel() for p in cnn.parameters())\n",
        "print(f\"Training SimpleCNN model with {total_params} parameters\")\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize the kernel weights and images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize first-layer kernels and their feature maps on a test image\n",
        "def plot_kernels(model, test_dl, i_test):\n",
        "    # get one batch from test loader and pick first image\n",
        "    imgs_test, labels_test = next(iter(test_dl))\n",
        "    img = imgs_test[i_test].to(device)  # shape (1,3,32,32)\n",
        "    # show the three channels side-by-side\n",
        "    img_cpu = img.squeeze(0).cpu()  # (3,H,W)\n",
        "    # show full RGB image (denormalize from Normalize((0.5,), (0.5,)))\n",
        "    img_rgb = img_cpu.permute(1, 2, 0).numpy()\n",
        "    img_rgb = np.clip(img_rgb * 0.5 + 0.5, 0, 1)\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(f\"Label: {testset.classes[labels_test[i_test]]}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.figure(figsize=(9, 3))\n",
        "\n",
        "    cmaps = [\"Reds\", \"Greens\", \"Blues\"]\n",
        "    for i in range(3):\n",
        "        ch = img_cpu[i].numpy()\n",
        "        ch = (ch - ch.min()) / (ch.max() - ch.min() + 1e-8)  # normalize for display\n",
        "        ax = plt.subplot(1, 3, i + 1)\n",
        "        ax.imshow(ch, cmap=cmaps[i])\n",
        "        ax.set_title(f\"channel {i}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.suptitle(\"Test image channels\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    conv1 = cnn.features[0]  # first Conv2d layer\n",
        "    kernels = conv1.weight.detach().cpu()  # (out_ch, in_ch, kH, kW)\n",
        "    out_ch = kernels.shape[0]\n",
        "\n",
        "    # number of kernels / feature maps to show\n",
        "    n_show = 32\n",
        "    n_cols = 8\n",
        "    n_rows = math.ceil(n_show / n_cols)\n",
        "\n",
        "    # plot kernels as RGB images (normalize per-kernel)\n",
        "    plt.figure(figsize=(n_cols * 2, n_rows * 2))\n",
        "    for i in range(n_show):\n",
        "        k = kernels[i]  # (3, kH, kW)\n",
        "        k_min, k_max = k.min(), k.max()\n",
        "        k_img = (k - k_min) / (k_max - k_min + 1e-8)  # normalize to 0-1\n",
        "        k_img = k_img.permute(1, 2, 0).numpy()  # H,W,C\n",
        "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "        ax.imshow(k_img)\n",
        "        ax.set_title(f\"kernel {i}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.suptitle(\"First-layer kernels (normalized RGB)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # compute feature maps produced by conv1 for the chosen test image\n",
        "    with torch.no_grad():\n",
        "        acts = conv1(img).squeeze(0).cpu()  # (out_ch, H, W)\n",
        "\n",
        "    # plot first n_show feature maps (grayscale)\n",
        "    plt.figure(figsize=(n_cols * 2, n_rows * 2))\n",
        "    for i in range(n_show):\n",
        "        act = acts[i].numpy()\n",
        "        # normalize each activation map for visualization\n",
        "        act = (act - act.min()) / (act.max() - act.min() + 1e-8)\n",
        "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "        ax.imshow(act, cmap=\"gray\")\n",
        "        ax.set_title(f\"Features {i}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.suptitle(\"Feature maps after first conv (normalized)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_weights = []\n",
        "with torch.no_grad():\n",
        "    for p in cnn.parameters():\n",
        "        model_weights.append(p.view(-1).abs())\n",
        "\n",
        "    model_weights = torch.cat(model_weights).numpy()\n",
        "\n",
        "\n",
        "plt.hist(model_weights, bins=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialization Matters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero Initialization\n",
        "Gradients are all the same for all weights in a layer: no learning can occur. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"zero_init\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "# initialize all weights and biases to zero\n",
        "for name, param in cnn.named_parameters():\n",
        "    param.data.zero_()\n",
        "print(\"Initialized all cnn parameters to zero.\")\n",
        "\n",
        "total_params = sum(p.numel() for p in cnn.parameters())\n",
        "print(f\"Training SimpleCNN model with {total_params} parameters\")\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Default Initialization\n",
        "\n",
        "Check pytorch default initialization for \n",
        "- conv2d layers\n",
        "- linear layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non-adaptive initialization schemes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"normal_init\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "\n",
        "def init_normal(m):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        nn.init.normal_(m.weight, mean=0.0, std=0.1)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "cnn.apply(init_normal)\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dropout Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our baseline model\n",
        "class CNN_dropout(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        layers = []\n",
        "        for i in range(5):\n",
        "            layers += [\n",
        "                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "            ]\n",
        "            if i in {1, 3, 4}:  # downsample after 2nd, 4th, and 5th conv\n",
        "                layers.append(nn.MaxPool2d(2, 2))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"dropout\"\n",
        "cnn_skip = CNN_dropout().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn_skip.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn_skip, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skip connections\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, downsample=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "        )\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2) if downsample else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out + self.skip(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNN_skip(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        downsamples = {1, 3, 4}  # same downsampling pattern as before\n",
        "        blocks = []\n",
        "        for i in range(5):\n",
        "            in_ch, out_ch = channels[i], channels[i + 1]\n",
        "            down = i in downsamples\n",
        "            blocks.append(ResidualBlock(in_ch, out_ch, downsample=down))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"skip\"\n",
        "cnn_skip = CNN_dropout().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn_skip.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn_skip, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explicit regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l1norm(model):\n",
        "    return sum([p.abs().sum() for p in model.parameters()])\n",
        "\n",
        "\n",
        "def l2norm(model):\n",
        "    return math.sqrt(sum([p.norm() ** 2 for p in model.parameters()]))\n",
        "\n",
        "\n",
        "def train_reg(\n",
        "    model,\n",
        "    train_dl,\n",
        "    opt,\n",
        "    criterion,\n",
        "    writer,\n",
        "    n_epochs,\n",
        "    name=\"\",\n",
        "    reg=\"l1\",\n",
        "    global_step=0,\n",
        "    prints=False,\n",
        "):\n",
        "    for epoch in range(n_epochs):\n",
        "        for imgs, labels in train_dl:\n",
        "            global_step += 1\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            unreg_loss = criterion(logits, labels)\n",
        "            if reg == \"l1\":\n",
        "                penalty = 1e-4 * l1norm(model)\n",
        "            elif reg == \"l2\":\n",
        "                penalty = 1e-3 * l2norm(model)\n",
        "            else:\n",
        "                penalty = 0\n",
        "            loss = unreg_loss + penalty\n",
        "            writer.add_scalar(\"train/cnn_loss\", unreg_loss.item(), global_step)\n",
        "            if global_step % 100 == 0:\n",
        "                log_grad_norms(model, writer, global_step, model_name=name)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        print(f\"Epoch {epoch + 1}: unregularized train loss = {unreg_loss.item():.4f}\")\n",
        "        print(f\"Epoch {epoch + 1}: full train loss = {loss.item():.4f}\")\n",
        "        if prints:\n",
        "            print(get_layer_grad_norms(model))\n",
        "        test_model(model, test_dl, criterion, writer=None, global_step=None, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"ell1_reg\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train_reg(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_weights_histogram(model):\n",
        "    model_weights = []\n",
        "    for p in model.parameters():\n",
        "        model_weights.append(p.view(-1).abs())\n",
        "\n",
        "    model_weights = torch.cat(model_weights).numpy()\n",
        "\n",
        "    plt.hist(model_weights, bins=100)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "cnn_init = CNN_base().to(device)\n",
        "\n",
        "\n",
        "plot_weights_histogram(cnn_init)\n",
        "plot_weights_histogram(cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# We want to augment only our subset of data for fair comparison\n",
        "class AugmentedSubset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "\n",
        "n_train = 200\n",
        "train_subset = Subset(trainset, list(range(n_train)))\n",
        "\n",
        "augmented_subset = AugmentedSubset(train_subset)\n",
        "aug_train_dl = DataLoader(augmented_subset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"ell1_reg\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, aug_train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Do you own experiments!\n",
        "Try to achieve the best possible test accuracy on CIFAR10 with these 200 points. \n",
        "You can use any of the tricks seen in this TP or others you may know of.\n",
        "\n",
        "Add more data points to the training set and see how the different tricks scale with more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class figure:  Learning rate scheduling illustration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "T = 200\n",
        "base_lr = 1e-3\n",
        "\n",
        "\n",
        "def make_optimizer():\n",
        "    model = torch.nn.Linear(10, 1)\n",
        "    return torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "\n",
        "\n",
        "optimizers = {name: make_optimizer() for name in range(5)}\n",
        "\n",
        "schedulers = {\n",
        "    \"Constant\": torch.optim.lr_scheduler.LambdaLR(make_optimizer(), lambda _: 1.0),\n",
        "    \"StepLR\": torch.optim.lr_scheduler.StepLR(\n",
        "        make_optimizer(), step_size=50, gamma=0.5\n",
        "    ),\n",
        "    \"ExponentialLR\": torch.optim.lr_scheduler.ExponentialLR(\n",
        "        make_optimizer(), gamma=0.98\n",
        "    ),\n",
        "    \"CosineAnnealingLR\": torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        make_optimizer(), T_max=T\n",
        "    ),\n",
        "    \"OneCycleLR\": torch.optim.lr_scheduler.OneCycleLR(\n",
        "        make_optimizer(), max_lr=1e-3, total_steps=T\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Record learning rates\n",
        "lrs = {name: [] for name in schedulers}\n",
        "\n",
        "for step in range(T):\n",
        "    for name, sched in schedulers.items():\n",
        "        opt = sched.optimizer\n",
        "        lrs[name].append(opt.param_groups[0][\"lr\"])\n",
        "        sched.step()\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "for name, values in lrs.items():\n",
        "    plt.plot(values, label=name)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"Typical Learning Rate Schedulers in PyTorch\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl_tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
