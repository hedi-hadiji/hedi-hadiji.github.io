{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TP5: Training Tricks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset and define a transformation done on all images:\n",
        "# - resize to 32x32 (does nothing here)\n",
        "# - convert to tensor\n",
        "# - normalize to mean 0.5, std 0.5 for each channel\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "trainset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "testset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "print(f\"Train points {len(trainset)}\")\n",
        "print(f\"Test points {len(testset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show a few images from the training loader\n",
        "train_dl = DataLoader(trainset, batch_size=8, shuffle=True)\n",
        "imgs, labels = next(iter(train_dl))\n",
        "imgs = imgs[:8] * 0.5 + 0.5\n",
        "grid = make_grid(imgs, nrow=4)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis(\"off\")\n",
        "plt.title(\", \".join([trainset.classes[int(ell)] for ell in labels[:8]]))\n",
        "plt.show()\n",
        "\n",
        "print(f\"Number of classes: {len(trainset.classes)}\")\n",
        "# print(f\"Classes: {trainset.classes}\")\n",
        "print(f\"Image shape: {imgs[0].shape}\")  # C, H, W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Our Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our baseline model\n",
        "class CNN_base(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        layers = []\n",
        "        for i in range(5):\n",
        "            layers += [\n",
        "                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "            ]\n",
        "            if i in {1, 3, 4}:  # downsample after 2nd, 4th, and 5th conv\n",
        "                layers.append(nn.MaxPool2d(2, 2))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n_train = 200\n",
        "train_subset = Subset(trainset, list(range(n_train)))\n",
        "train_dl = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(testset, batch_size=4)\n",
        "\n",
        "writer = SummaryWriter(\"runs/vit_cifar_experiment\")\n",
        "\n",
        "# log a small batch of images and the model graph (if possible)\n",
        "imgs_sample, labels_sample = next(iter(train_dl))\n",
        "imgs_sample = imgs_sample.to(device)\n",
        "# writer.add_graph(model, imgs_sample)  # may fail for some models\n",
        "grid = make_grid(imgs_sample[:16], nrow=4, normalize=True, scale_each=True)\n",
        "writer.add_image(\"train/sample_images\", grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Our Training Set \n",
        "We focus on a tiny subset of CIFAR to test optimization on a simplet setup.\n",
        "Normally, we should end up overfitting this small training set quickly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = 200\n",
        "train_subset = Subset(trainset, list(range(n_train)))\n",
        "train_dl = DataLoader(train_subset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some utility functions to log info during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_grad_norms(model, writer, step, model_name=\"\"):\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is not None:\n",
        "            writer.add_scalar(f\"{model_name}/grads/{name}\", p.grad.norm().item(), step)\n",
        "\n",
        "\n",
        "def get_layer_grad_norms(model):\n",
        "    layer_norms = defaultdict(float)\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        layer_norms[name] += p.grad.norm().item() ** 2\n",
        "    return {k: v**0.5 for k, v in layer_norms.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, test_dl, criterion, writer=None, global_step=None, name=\"\"):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_dl:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = model(imgs)\n",
        "\n",
        "            losses.append(criterion(logits, labels).mean().numpy())\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    if writer is not None and global_step is not None:\n",
        "        writer.add_scalar(\"name/test/accuracy\", 100 * correct / total, global_step)\n",
        "        writer.add_scalar(\"name/test/loss\", np.mean(losses), global_step)\n",
        "    print(f\"Test accuracy: {100 * correct / total:.2f}%, test loss: {np.mean(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_dl,\n",
        "    opt,\n",
        "    criterion,\n",
        "    writer,\n",
        "    n_epochs,\n",
        "    name=\"\",\n",
        "    global_step=0,\n",
        "    prints=False,\n",
        "):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for imgs, labels in train_dl:\n",
        "            global_step += 1\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            writer.add_scalar(\"train/cnn_loss\", loss.item(), global_step)\n",
        "            if global_step % 100 == 0:\n",
        "                log_grad_norms(model, writer, global_step, model_name=name)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        print(f\"Epoch {epoch + 1}: train loss = {loss.item():.4f}\")\n",
        "        if prints:\n",
        "            print(get_layer_grad_norms(model))\n",
        "        model.eval()\n",
        "        test_model(model, test_dl, criterion, writer=None, global_step=None, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try sigmoid\n",
        "class CNN5_sigmoid(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        layers = []\n",
        "        for i in range(5):\n",
        "            layers += [\n",
        "                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),\n",
        "                nn.Sigmoid(),\n",
        "            ]\n",
        "            if i in {1, 3, 4}:  # downsample after 2nd, 4th, and 5th conv\n",
        "                layers.append(nn.MaxPool2d(2, 2))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = 200\n",
        "train_subset = Subset(trainset, range(n_train))\n",
        "train_dl = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "cnn_sig = CNN5_sigmoid().to(device)\n",
        "total_params = sum(p.numel() for p in cnn_sig.parameters())\n",
        "print(f\"Training SimpleCNN model with {total_params} parameters\")\n",
        "\n",
        "opt = torch.optim.Adam(cnn_sig.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "name = \"sig_act\"\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn_sig, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare to initial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"baseline\"\n",
        "cnn = CNN_base().to(device)\n",
        "total_params = sum(p.numel() for p in cnn.parameters())\n",
        "print(f\"Training SimpleCNN model with {total_params} parameters\")\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize the kernel weights and images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize first-layer kernels and their feature maps on a test image\n",
        "\n",
        "# get one batch from test loader and pick first image\n",
        "imgs_test, labels_test = next(iter(test_dl))\n",
        "i_test = 2\n",
        "img = imgs_test[i_test].to(device)  # shape (1,3,32,32)\n",
        "# show the three channels side-by-side\n",
        "img_cpu = img.squeeze(0).cpu()  # (3,H,W)\n",
        "# show full RGB image (denormalize from Normalize((0.5,), (0.5,)))\n",
        "img_rgb = img_cpu.permute(1, 2, 0).numpy()\n",
        "img_rgb = np.clip(img_rgb * 0.5 + 0.5, 0, 1)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(img_rgb)\n",
        "plt.title(f\"Label: {testset.classes[labels_test[i_test]]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.figure(figsize=(9, 3))\n",
        "\n",
        "cmaps = [\"Reds\", \"Greens\", \"Blues\"]\n",
        "for i in range(3):\n",
        "    ch = img_cpu[i].numpy()\n",
        "    ch = (ch - ch.min()) / (ch.max() - ch.min() + 1e-8)  # normalize for display\n",
        "    ax = plt.subplot(1, 3, i + 1)\n",
        "    ax.imshow(ch, cmap=cmaps[i])\n",
        "    ax.set_title(f\"channel {i}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Test image channels\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "conv1 = cnn.features[0]  # first Conv2d layer\n",
        "kernels = conv1.weight.detach().cpu()  # (out_ch, in_ch, kH, kW)\n",
        "out_ch = kernels.shape[0]\n",
        "\n",
        "# number of kernels / feature maps to show\n",
        "n_show = 32\n",
        "n_cols = 8\n",
        "n_rows = math.ceil(n_show / n_cols)\n",
        "\n",
        "# plot kernels as RGB images (normalize per-kernel)\n",
        "plt.figure(figsize=(n_cols * 2, n_rows * 2))\n",
        "for i in range(n_show):\n",
        "    k = kernels[i]  # (3, kH, kW)\n",
        "    k_min, k_max = k.min(), k.max()\n",
        "    k_img = (k - k_min) / (k_max - k_min + 1e-8)  # normalize to 0-1\n",
        "    k_img = k_img.permute(1, 2, 0).numpy()  # H,W,C\n",
        "    ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "    ax.imshow(k_img)\n",
        "    ax.set_title(f\"kernel {i}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"First-layer kernels (normalized RGB)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# compute feature maps produced by conv1 for the chosen test image\n",
        "with torch.no_grad():\n",
        "    acts = conv1(img).squeeze(0).cpu()  # (out_ch, H, W)\n",
        "\n",
        "# plot first n_show feature maps (grayscale)\n",
        "plt.figure(figsize=(n_cols * 2, n_rows * 2))\n",
        "for i in range(n_show):\n",
        "    act = acts[i].numpy()\n",
        "    # normalize each activation map for visualization\n",
        "    act = (act - act.min()) / (act.max() - act.min() + 1e-8)\n",
        "    ax = plt.subplot(n_rows, n_cols, i + 1)\n",
        "    ax.imshow(act, cmap=\"gray\")\n",
        "    ax.set_title(f\"Features {i}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Feature maps after first conv (normalized)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_weights = []\n",
        "with torch.no_grad():\n",
        "    for p in cnn.parameters():\n",
        "        model_weights.append(p.view(-1).abs())\n",
        "\n",
        "    model_weights = torch.cat(model_weights).numpy()\n",
        "\n",
        "\n",
        "plt.hist(model_weights, bins=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialization Matters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero Initialization\n",
        "Gradients are all the same for all weights in a layer: no learning can occur. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"zero_init\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "# initialize all weights and biases to zero\n",
        "for name, param in cnn.named_parameters():\n",
        "    param.data.zero_()\n",
        "print(\"Initialized all cnn parameters to zero.\")\n",
        "\n",
        "total_params = sum(p.numel() for p in cnn.parameters())\n",
        "print(f\"Training SimpleCNN model with {total_params} parameters\")\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Default Initialization\n",
        "\n",
        "Check pytorch default initialization for \n",
        "- conv2d layers\n",
        "- linear layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non-adaptive initialization schemes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"normal_init\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "\n",
        "def init_normal(m):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        nn.init.normal_(m.weight, mean=0.0, std=0.1)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "cnn.apply(init_normal)\n",
        "\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dropout Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our baseline model\n",
        "class CNN_dropout(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        layers = []\n",
        "        for i in range(5):\n",
        "            layers += [\n",
        "                nn.Conv2d(channels[i], channels[i + 1], 3, padding=1),\n",
        "                nn.ReLU(),\n",
        "            ]\n",
        "            if i in {1, 3, 4}:  # downsample after 2nd, 4th, and 5th conv\n",
        "                layers.append(nn.MaxPool2d(2, 2))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"dropout\"\n",
        "cnn_skip = CNN_dropout().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn_skip.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn_skip, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skip connections\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, downsample=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "        )\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2) if downsample else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out + self.skip(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNN_skip(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        channels = [3, 32, 64, 128, 128, 256]\n",
        "        downsamples = {1, 3, 4}  # same downsampling pattern as before\n",
        "        blocks = []\n",
        "        for i in range(5):\n",
        "            in_ch, out_ch = channels[i], channels[i + 1]\n",
        "            down = i in downsamples\n",
        "            blocks.append(ResidualBlock(in_ch, out_ch, downsample=down))\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"skip\"\n",
        "cnn_skip = CNN_dropout().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn_skip.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn_skip, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explicit regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l1norm(model):\n",
        "    return sum([p.abs().sum() for p in model.parameters()])\n",
        "\n",
        "\n",
        "def l2norm(model):\n",
        "    return math.sqrt(sum([p.norm() ** 2 for p in model.parameters()]))\n",
        "\n",
        "\n",
        "def train_reg(\n",
        "    model,\n",
        "    train_dl,\n",
        "    opt,\n",
        "    criterion,\n",
        "    writer,\n",
        "    n_epochs,\n",
        "    name=\"\",\n",
        "    reg=\"l1\",\n",
        "    global_step=0,\n",
        "    prints=False,\n",
        "):\n",
        "    for epoch in range(n_epochs):\n",
        "        for imgs, labels in train_dl:\n",
        "            global_step += 1\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            unreg_loss = criterion(logits, labels)\n",
        "            if reg == \"l1\":\n",
        "                penalty = 1e-4 * l1norm(model)\n",
        "            elif reg == \"l2\":\n",
        "                penalty = 1e-3 * l2norm(model)\n",
        "            else:\n",
        "                penalty = 0\n",
        "            loss = unreg_loss + penalty\n",
        "            writer.add_scalar(\"train/cnn_loss\", unreg_loss.item(), global_step)\n",
        "            if global_step % 100 == 0:\n",
        "                log_grad_norms(model, writer, global_step, model_name=name)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        print(f\"Epoch {epoch + 1}: unregularized train loss = {unreg_loss.item():.4f}\")\n",
        "        print(f\"Epoch {epoch + 1}: full train loss = {loss.item():.4f}\")\n",
        "        if prints:\n",
        "            print(get_layer_grad_norms(model))\n",
        "        test_model(model, test_dl, criterion, writer=None, global_step=None, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"ell1_reg\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train_reg(cnn, train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_weights_histogram(model):\n",
        "    model_weights = []\n",
        "    for p in model.parameters():\n",
        "        model_weights.append(p.view(-1).abs())\n",
        "\n",
        "    model_weights = torch.cat(model_weights).numpy()\n",
        "\n",
        "    plt.hist(model_weights, bins=100)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "cnn_init = CNN_base().to(device)\n",
        "\n",
        "\n",
        "plot_weights_histogram(cnn_init)\n",
        "plot_weights_histogram(cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# We want to augment only our subset of data for fair comparison\n",
        "class AugmentedSubset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "\n",
        "n_train = 200\n",
        "train_subset = Subset(trainset, list(range(n_train)))\n",
        "\n",
        "augmented_subset = AugmentedSubset(train_subset)\n",
        "aug_train_dl = DataLoader(augmented_subset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train loss = 2.3515\n",
            "Test accuracy: 10.00%, test loss: 2.3001904487609863\n",
            "Epoch 2: train loss = 2.2516\n",
            "Test accuracy: 10.00%, test loss: 2.2943217754364014\n",
            "Epoch 3: train loss = 2.0315\n",
            "Test accuracy: 10.01%, test loss: 2.2843575477600098\n",
            "Epoch 4: train loss = 2.2184\n",
            "Test accuracy: 11.42%, test loss: 2.2363505363464355\n",
            "Epoch 5: train loss = 2.2361\n",
            "Test accuracy: 18.91%, test loss: 2.1686460971832275\n",
            "Epoch 6: train loss = 1.9335\n",
            "Test accuracy: 24.47%, test loss: 2.137650489807129\n",
            "Epoch 7: train loss = 1.7620\n",
            "Test accuracy: 22.81%, test loss: 2.1386733055114746\n",
            "Epoch 8: train loss = 1.7518\n",
            "Test accuracy: 26.12%, test loss: 2.133864402770996\n",
            "Epoch 9: train loss = 1.1876\n",
            "Test accuracy: 24.52%, test loss: 2.236203193664551\n",
            "Epoch 10: train loss = 1.4397\n",
            "Test accuracy: 25.56%, test loss: 2.301499366760254\n",
            "Epoch 11: train loss = 1.6409\n",
            "Test accuracy: 24.36%, test loss: 2.3946194648742676\n",
            "Epoch 12: train loss = 1.6840\n",
            "Test accuracy: 25.01%, test loss: 2.516458511352539\n",
            "Epoch 13: train loss = 1.1342\n",
            "Test accuracy: 23.92%, test loss: 2.5584871768951416\n",
            "Epoch 14: train loss = 1.0729\n",
            "Test accuracy: 24.83%, test loss: 2.6799426078796387\n",
            "Epoch 15: train loss = 1.9063\n",
            "Test accuracy: 23.40%, test loss: 2.8965117931365967\n",
            "Epoch 16: train loss = 1.0227\n",
            "Test accuracy: 25.88%, test loss: 2.7189927101135254\n",
            "Epoch 17: train loss = 0.9764\n",
            "Test accuracy: 26.03%, test loss: 2.7699663639068604\n",
            "Epoch 18: train loss = 0.9613\n",
            "Test accuracy: 26.12%, test loss: 3.255354642868042\n",
            "Epoch 19: train loss = 0.6177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 244, in run\n",
            "    self._run()\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 275, in _run\n",
            "    self._record_writer.write(data)\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
            "    self._writer.write(header + header_crc + data + footer_crc)\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 773, in write\n",
            "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 167, in append\n",
            "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
            "  File \"/Users/hadiji/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 171, in _write\n",
            "    with io.open(filename, mode, encoding=encoding) as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: b'runs/vit_cifar_experiment/events.out.tfevents.1762246680.MacBook-Pro-de-Cellule.local.84193.0'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 24.14%, test loss: 3.9407131671905518\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: b'runs/vit_cifar_experiment/events.out.tfevents.1762246680.MacBook-Pro-de-Cellule.local.84193.0'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 10\u001b[0m train(cnn, aug_train_dl, opt, criterion, writer, n_epochs, name\u001b[38;5;241m=\u001b[39mname, global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, opt, criterion, writer, n_epochs, name, global_step, prints)\u001b[0m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[0;32m---> 19\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/cnn_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), global_step)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     log_grad_norms(model, writer, global_step, model_name\u001b[38;5;241m=\u001b[39mname)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:396\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    391\u001b[0m     scalar_value \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(scalar_value)\n\u001b[1;32m    393\u001b[0m summary \u001b[38;5;241m=\u001b[39m scalar(\n\u001b[1;32m    394\u001b[0m     tag, scalar_value, new_style\u001b[38;5;241m=\u001b[39mnew_style, double_precision\u001b[38;5;241m=\u001b[39mdouble_precision\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:114\u001b[0m, in \u001b[0;36mFileWriter.add_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    walltime (from time.time()) seconds after epoch\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m event \u001b[38;5;241m=\u001b[39m event_pb2\u001b[38;5;241m.\u001b[39mEvent(summary\u001b[38;5;241m=\u001b[39msummary)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_event(event, global_step, walltime)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:98\u001b[0m, in \u001b[0;36mFileWriter.add_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     event\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(step)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_writer\u001b[38;5;241m.\u001b[39madd_event(event)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:117\u001b[0m, in \u001b[0;36mEventFileWriter.add_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, event_pb2\u001b[38;5;241m.\u001b[39mEvent):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an event_pb2.Event proto, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(event)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_writer\u001b[38;5;241m.\u001b[39mwrite(event\u001b[38;5;241m.\u001b[39mSerializeToString())\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:171\u001b[0m, in \u001b[0;36m_AsyncWriter.write\u001b[0;34m(self, bytestring)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Enqueue the given bytes to be written asychronously.\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Status of the worker should be checked under the lock to avoid\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# multiple threads passing the check and then switching just before\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# blocking on putting to the queue which might result in a deadlock.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_worker_status()\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriter is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:212\u001b[0m, in \u001b[0;36m_AsyncWriter._check_worker_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/threading.py:1045\u001b[0m, in \u001b[0;36mThread._bootstrap_inner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     _sys\u001b[38;5;241m.\u001b[39msetprofile(_profile_hook)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_excepthook(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:244\u001b[0m, in \u001b[0;36m_AsyncWriterThread.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m ex\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:275\u001b[0m, in \u001b[0;36m_AsyncWriterThread._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_signal:\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_writer\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_pending_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/summary/writer/record_writer.py:40\u001b[0m, in \u001b[0;36mRecordWriter.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m header_crc \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, masked_crc32c(header))\n\u001b[1;32m     39\u001b[0m footer_crc \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, masked_crc32c(data))\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mwrite(header \u001b[38;5;241m+\u001b[39m header_crc \u001b[38;5;241m+\u001b[39m data \u001b[38;5;241m+\u001b[39m footer_crc)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:773\u001b[0m, in \u001b[0;36mGFile.write\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;66;03m# append the later chunks\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, file_content, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_mode)\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;66;03m# add to temp file, but wait for flush to write to final filesystem\u001b[39;00m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_temp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:167\u001b[0m, in \u001b[0;36mLocalFileSystem.append\u001b[0;34m(self, filename, file_content, binary_mode)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, binary_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Append string file contents to a file.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m        binary_mode: bool, write as binary if True, otherwise text\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write(filename, file_content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m binary_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:171\u001b[0m, in \u001b[0;36mLocalFileSystem._write\u001b[0;34m(self, filename, file_content, mode)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, mode):\n\u001b[1;32m    170\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(filename, mode, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    172\u001b[0m         compatify \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_bytes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mas_text\n\u001b[1;32m    173\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(compatify(file_content))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'runs/vit_cifar_experiment/events.out.tfevents.1762246680.MacBook-Pro-de-Cellule.local.84193.0'"
          ]
        }
      ],
      "source": [
        "name = \"ell1_reg\"\n",
        "cnn = CNN_base().to(device)\n",
        "\n",
        "opt = torch.optim.Adam(cnn.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "global_step = 0\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "train(cnn, aug_train_dl, opt, criterion, writer, n_epochs, name=name, global_step=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Do you own experiments!\n",
        "Try to achieve the best possible test accuracy on CIFAR10 with these 200 points. \n",
        "You can use any of the tricks seen in this TP or others you may know of.\n",
        "\n",
        "Add more data points to the training set and see how the different tricks scale with more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class figure:  Learning rate scheduling illustration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "T = 200\n",
        "base_lr = 1e-3\n",
        "\n",
        "\n",
        "def make_optimizer():\n",
        "    model = torch.nn.Linear(10, 1)\n",
        "    return torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "\n",
        "\n",
        "optimizers = {name: make_optimizer() for name in range(5)}\n",
        "\n",
        "schedulers = {\n",
        "    \"Constant\": torch.optim.lr_scheduler.LambdaLR(make_optimizer(), lambda _: 1.0),\n",
        "    \"StepLR\": torch.optim.lr_scheduler.StepLR(\n",
        "        make_optimizer(), step_size=50, gamma=0.5\n",
        "    ),\n",
        "    \"ExponentialLR\": torch.optim.lr_scheduler.ExponentialLR(\n",
        "        make_optimizer(), gamma=0.98\n",
        "    ),\n",
        "    \"CosineAnnealingLR\": torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        make_optimizer(), T_max=T\n",
        "    ),\n",
        "    \"OneCycleLR\": torch.optim.lr_scheduler.OneCycleLR(\n",
        "        make_optimizer(), max_lr=1e-3, total_steps=T\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Record learning rates\n",
        "lrs = {name: [] for name in schedulers}\n",
        "\n",
        "for step in range(T):\n",
        "    for name, sched in schedulers.items():\n",
        "        opt = sched.optimizer\n",
        "        lrs[name].append(opt.param_groups[0][\"lr\"])\n",
        "        sched.step()\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "for name, values in lrs.items():\n",
        "    plt.plot(values, label=name)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"Typical Learning Rate Schedulers in PyTorch\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl_tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
