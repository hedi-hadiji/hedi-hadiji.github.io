{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TP6 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA on CIFAR\n",
        "\n",
        "We plot the results of PCA on CIFAR-10 and MNIST datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_pca_and_reconstruct(X, k, channels, side):\n",
        "    pca = PCA(n_components=k)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    X_rec = pca.inverse_transform(X_pca)\n",
        "    X_rec = torch.tensor(X_rec).view(-1, channels, side, side)\n",
        "    X_rem = torch.tensor(X).view(-1, channels, side, side) - X_rec\n",
        "    return X_rec, X_rem, pca\n",
        "\n",
        "\n",
        "# Show originals vs reconstructions\n",
        "def show_pair(orig, rec, rem, n=5):\n",
        "    plt.figure(figsize=(2 * n, 4))\n",
        "    for i in range(n):\n",
        "        plt.subplot(3, n, i + 1)\n",
        "        plt.imshow(orig[i].permute(1, 2, 0))\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(3, n, n + i + 1)\n",
        "        plt.imshow(rec[i].permute(1, 2, 0).clamp(0, 1))\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(3, n, 2 * n + i + 1)\n",
        "        plt.imshow(rem[i].permute(1, 2, 0).clamp(0, 1))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load CIFAR-10\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(\n\u001b[1;32m      4\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m      7\u001b[0m     [x \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(dataset, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m))]\n\u001b[1;32m      8\u001b[0m )  \u001b[38;5;66;03m# 10k images\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "images = torch.stack(\n",
        "    [x for x, _ in torch.utils.data.Subset(dataset, range(10000))]\n",
        ")  # 10k images\n",
        "X = images.view(len(images), -1).numpy()  # flatten to (N, 3072)\n",
        "channels = 3\n",
        "side = 32\n",
        "\n",
        "# Fit PCA\n",
        "k = 10\n",
        "X_rec, X_rem, pca = fit_pca_and_reconstruct(X, k, channels, side)\n",
        "show_pair(images, X_rec, X_rem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 15 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "images = torch.stack(\n",
        "    [x for x, _ in torch.utils.data.Subset(dataset, range(10000))]\n",
        ")  # 10k images\n",
        "X = images.view(len(images), -1).numpy()\n",
        "channels = 1\n",
        "side = 28\n",
        "\n",
        "\n",
        "# Fit PCA\n",
        "k = 10\n",
        "X_rec, X_rem, pca = fit_pca_and_reconstruct(X, k, channels, side)\n",
        "show_pair(images, X_rec, X_rem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Autoencoder\n",
        "\n",
        "Build a simple autoencoder and train it on MNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50 | MSE: 6.73537\n",
            "Epoch 20/50 | MSE: 6.35151\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     50\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb, _ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     52\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     53\u001b[0m         x_hat, _ \u001b[38;5;241m=\u001b[39m model(xb)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
            "File \u001b[0;32m~/anaconda3/envs/dl_tutorial/lib/python3.11/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, d_in=28**2, d_latent=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(d_in, 16),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, d_latent),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_latent, 16),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, d_in),\n",
        "            nn.Sigmoid(),  # outputs in [0,1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : (B, 1, 28, 28)\n",
        "        # need to flatten to give as input to the encoder\n",
        "        # return both reconstructed x and latent z\n",
        "\n",
        "        z = self.encode(x.view(-1, 28 * 28))\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, z\n",
        "\n",
        "\n",
        "model = AutoEncoder(d_in=28**2, d_latent=2).to(device)\n",
        "\n",
        "# Loss & optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(\".\", train=True, download=True, transform=transform),\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for xb, _ in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x_hat, _ = model(xb)\n",
        "        x_hat = x_hat.reshape(-1, 1, 28, 28)\n",
        "        loss = criterion(x_hat, xb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1:02d}/{epochs} | MSE: {epoch_loss / len(train_loader):.5f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(\".\", train=False, download=True, transform=transform),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "X_test, y_test = list(test_loader)[0]\n",
        "\n",
        "# ----- Reconstruct a small test batch -----\n",
        "with torch.no_grad():\n",
        "    x_hat_test, z_test = model(X_test)\n",
        "\n",
        "# Pick 10 examples to display\n",
        "n_show = 10\n",
        "idx = np.arange(len(X_test))\n",
        "np.random.shuffle(idx)\n",
        "idx = idx[:n_show]\n",
        "\n",
        "orig = X_test[idx].numpy().reshape(n_show, 28, 28)\n",
        "reco = x_hat_test[idx].numpy().reshape(n_show, 28, 28)\n",
        "\n",
        "# Grid of originals\n",
        "fig, axes = plt.subplots(2, n_show, figsize=(n_show * 1.1, 2.4))\n",
        "for i in range(n_show):\n",
        "    axes[0, i].imshow(orig[i], cmap=\"gray_r\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    axes[1, i].imshow(reco[i], cmap=\"gray_r\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "axes[0, 0].set_title(\"Originals\")\n",
        "axes[1, 0].set_title(\"Reconstructions\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2D latent space scatter colored by class\n",
        "Z = z_test.numpy()\n",
        "Y = y_test.numpy()\n",
        "\n",
        "plt.figure(figsize=(5.5, 5.0))\n",
        "for d in range(10):\n",
        "    mask = Y == d\n",
        "    plt.scatter(Z[mask, 0], Z[mask, 1], s=12, label=str(d), alpha=0.8)\n",
        "plt.xlabel(\"z1\")\n",
        "plt.ylabel(\"z2\")\n",
        "plt.title(\"2D Latent Space (test data)\")\n",
        "plt.legend(markerscale=1.2, fontsize=8, ncol=2, frameon=True)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decode some random points in the latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_size = 4\n",
        "with torch.no_grad():\n",
        "    z = 10 * torch.randn(grid_size**2, 2).to(device) + torch.tensor([5, 5])\n",
        "    samples = model.decode(z).cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "grid = torch.cat(\n",
        "    [\n",
        "        torch.cat(list(samples[i * grid_size : (i + 1) * grid_size]), dim=2)\n",
        "        for i in range(grid_size)\n",
        "    ],\n",
        "    dim=1,\n",
        ")\n",
        "plt.imshow(grid.squeeze(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variational AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=2):\n",
        "        super().__init__()\n",
        "        self.enc1 = nn.Linear(28 * 28, 400)\n",
        "        self.enc_mu = nn.Linear(400, latent_dim)\n",
        "        self.enc_logvar = nn.Linear(400, latent_dim)\n",
        "\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 28 * 28),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.enc1(x))\n",
        "        return self.enc_mu(h), self.enc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.dec(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 28 * 28))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar\n",
        "\n",
        "\n",
        "def KL(mu, logvar):\n",
        "    # KL divergence between N(mu, var) and N(0,1)\n",
        "    return 0.5 * torch.sum(mu**2 + logvar.exp() - logvar - 1)\n",
        "\n",
        "\n",
        "def vae_loss(x, x_hat, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(x_hat, x.view(-1, 28 * 28), reduction=\"sum\")\n",
        "    KLD = KL(mu, logvar)\n",
        "    return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_dim = 2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vae = VAE(latent_dim=latent_dim).to(device)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(\".\", train=True, download=True, transform=transform),\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "for epoch in range(20):\n",
        "    vae.train()\n",
        "    total_loss = 0\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        x_hat, mu, logvar = vae(x)\n",
        "        loss = vae_loss(x, x_hat, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sampling and visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_size = 6\n",
        "with torch.no_grad():\n",
        "    z = torch.ones(grid_size**2, latent_dim).to(device)\n",
        "    samples = vae.decode(z).cpu().view(-1, 1, 28, 28)\n",
        "\n",
        "grid = torch.cat(\n",
        "    [\n",
        "        torch.cat(list(samples[i * grid_size : (i + 1) * grid_size]), dim=2)\n",
        "        for i in range(grid_size)\n",
        "    ],\n",
        "    dim=1,\n",
        ")\n",
        "plt.imshow(grid.squeeze(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_show = 10\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(\".\", train=False, download=True, transform=transform),\n",
        "    batch_size=n_show,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    x, _ = list(test_loader)[10]\n",
        "    x_hat, _, _ = vae(x)\n",
        "    x, x_hat = x.cpu(), x_hat.cpu()\n",
        "\n",
        "orig = x[:n_show]\n",
        "recon = x[:n_show]\n",
        "\n",
        "fig, axes = plt.subplots(2, n_show, figsize=(n_show * 1.2, 2.6))\n",
        "for i in range(n_show):\n",
        "    axes[0, i].imshow(orig[i].squeeze(), cmap=\"gray_r\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    axes[1, i].imshow(recon[i].squeeze(), cmap=\"gray_r\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "axes[0, 0].set_title(\"Originals\")\n",
        "axes[1, 0].set_title(\"Reconstructions\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @dataclass\n",
        "# class MAEConfig:\n",
        "#     img_size: int = 224\n",
        "#     patch_size: int = 16\n",
        "#     in_chans: int = 3\n",
        "#     embed_dim: int = 512\n",
        "#     depth: int = 8\n",
        "#     num_heads: int = 8\n",
        "#     mask_ratio: float = 0.75\n",
        "#     decoder_dim: int = 256\n",
        "#     decoder_depth: int = 4\n",
        "#     decoder_heads: int = 8\n",
        "\n",
        "\n",
        "# class PatchEmbed(nn.Module):\n",
        "#     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "#         super().__init__()\n",
        "#         self.proj = nn.Conv2d(\n",
        "#             in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.proj(x)  # (B, D, H/P, W/P)\n",
        "#         x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, dim, depth, num_heads):\n",
        "#         super().__init__()\n",
        "#         layer = nn.TransformerEncoderLayer(\n",
        "#             d_model=dim,\n",
        "#             nhead=num_heads,\n",
        "#             dim_feedforward=dim * 4,\n",
        "#             activation=\"gelu\",\n",
        "#             batch_first=True,\n",
        "#             norm_first=True,\n",
        "#         )\n",
        "#         self.encoder = nn.TransformerEncoder(layer, num_layers=depth)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.encoder(x)\n",
        "\n",
        "\n",
        "# class TransformerDecoder(nn.Module):\n",
        "#     def __init__(self, dim, depth, num_heads):\n",
        "#         super().__init__()\n",
        "#         layer = nn.TransformerEncoderLayer(\n",
        "#             d_model=dim,\n",
        "#             nhead=num_heads,\n",
        "#             dim_feedforward=dim * 4,\n",
        "#             activation=\"gelu\",\n",
        "#             batch_first=True,\n",
        "#             norm_first=True,\n",
        "#         )\n",
        "#         self.decoder = nn.TransformerEncoder(layer, num_layers=depth)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.decoder(x)\n",
        "\n",
        "\n",
        "# class MaskedAutoencoderViT(nn.Module):\n",
        "#     def __init__(self, cfg: MAEConfig):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.patch_embed = PatchEmbed(\n",
        "#             cfg.img_size, cfg.patch_size, cfg.in_chans, cfg.embed_dim\n",
        "#         )\n",
        "#         self.num_patches = (cfg.img_size // cfg.patch_size) ** 2\n",
        "\n",
        "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.embed_dim))\n",
        "#         self.pos_embed = nn.Parameter(\n",
        "#             torch.zeros(1, self.num_patches + 1, cfg.embed_dim)\n",
        "#         )\n",
        "\n",
        "#         self.encoder = TransformerEncoder(cfg.embed_dim, cfg.depth, cfg.num_heads)\n",
        "#         # print(\"Loss:\", loss.item())\n",
        "\n",
        "#         self.mask_token = nn.Parameter(torch.zeros(1, 1, cfg.decoder_dim))\n",
        "#         self.decoder_embed = nn.Linear(cfg.embed_dim, cfg.decoder_dim)\n",
        "#         self.decoder_pos_embed = nn.Parameter(\n",
        "#             torch.zeros(1, self.num_patches + 1, cfg.decoder_dim)\n",
        "#         )\n",
        "#         self.decoder = TransformerDecoder(\n",
        "#             cfg.decoder_dim, cfg.decoder_depth, cfg.decoder_heads\n",
        "#         )\n",
        "#         self.head = nn.Linear(cfg.decoder_dim, cfg.patch_size**2 * cfg.in_chans)\n",
        "\n",
        "#         nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "#         nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def random_masking(self, x, mask_ratio):\n",
        "#         B, N, D = x.shape\n",
        "#         len_keep = int(N * (1 - mask_ratio))\n",
        "#         noise = torch.rand(B, N, device=x.device)\n",
        "#         ids_shuffle = torch.argsort(noise, dim=1)\n",
        "#         ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "#         ids_keep = ids_shuffle[:, :len_keep]\n",
        "#         x_masked = torch.gather(x, 1, ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "#         mask = torch.ones(B, N, device=x.device)\n",
        "#         mask[:, :len_keep] = 0\n",
        "#         mask = torch.gather(mask, 1, ids_restore)\n",
        "#         return x_masked, mask, ids_restore\n",
        "\n",
        "#     def forward(self, imgs):\n",
        "#         x = self.patch_embed(imgs)\n",
        "#         B, N, D = x.shape\n",
        "#         cls_token = self.cls_token.expand(B, -1, -1)\n",
        "#         x = torch.cat([cls_token, x], dim=1)\n",
        "#         x = x + self.pos_embed[:, : N + 1]\n",
        "\n",
        "#         x_cls, x_patches = x[:, :1, :], x[:, 1:, :]\n",
        "#         x_masked, mask, ids_restore = self.random_masking(\n",
        "#             x_patches, self.cfg.mask_ratio\n",
        "#         )\n",
        "#         x = torch.cat([x_cls, x_masked], dim=1)\n",
        "\n",
        "#         x = self.encoder(x)\n",
        "#         latent = x[:, 0]\n",
        "\n",
        "#         # Decode\n",
        "#         x = self.decoder_embed(x)\n",
        "#         x_cls, x_patches = x[:, :1, :], x[:, 1:, :]\n",
        "#         B = x.shape[0]\n",
        "#         mask_tokens = self.mask_token.repeat(\n",
        "#             B, self.num_patches - x_patches.shape[1], 1\n",
        "#         )\n",
        "#         x_full = torch.cat([x_patches, mask_tokens], dim=1)\n",
        "#         x_full = torch.gather(\n",
        "#             x_full, 1, ids_restore.unsqueeze(-1).repeat(1, 1, x_full.shape[-1])\n",
        "#         )\n",
        "#         x_full = torch.cat([x_cls, x_full], dim=1)\n",
        "#         x_full = x_full + self.decoder_pos_embed[:, : self.num_patches + 1]\n",
        "#         x_full = self.decoder(x_full)\n",
        "#         pred = self.head(x_full[:, 1:, :])\n",
        "\n",
        "#         # Compute loss\n",
        "#         p = self.cfg.patch_size\n",
        "#         with torch.no_grad():\n",
        "#             target = F.unfold(imgs, kernel_size=p, stride=p).transpose(1, 2)\n",
        "#             loss = (pred - target) ** 2\n",
        "#             loss = (loss.mean(-1) * mask).sum() / mask.sum()\n",
        "#             return loss, pred, mask\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def encode(self, imgs):\n",
        "#         x = self.patch_embed(imgs)\n",
        "#         B, N, D = x.shape\n",
        "#         cls_token = self.cls_token.expand(B, -1, -1)\n",
        "#         x = torch.cat([cls_token, x], dim=1)\n",
        "#         x = x + self.pos_embed[:, : N + 1]\n",
        "#         x = self.encoder(x)\n",
        "#         return x[:, 0]\n",
        "\n",
        "\n",
        "# cfg = MAEConfig(img_size=128, patch_size=16)\n",
        "# model = MaskedAutoencoderViT(cfg)\n",
        "# imgs = torch.randn(4, 3, cfg.img_size, cfg.img_size)\n",
        "# loss, _, _ = model(imgs)\n",
        "# print(\"Loss:\", loss.item())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl_tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
