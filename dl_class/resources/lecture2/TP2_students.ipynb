{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : Pytorch and convolutional neural nets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics of pytorch:\n",
    "- tensors (requires_grad, detach, item): activate the computation graph\n",
    "- modules: implement forward\n",
    "- parameters: registered parameters in module\n",
    "- nn.optimizer\n",
    "\n",
    "First step of debugging is paying attention to tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very similar syntax to numpy.\n",
    "zero_torch = torch.zeros((3, 2))\n",
    "\n",
    "print(\"zero_torch is of type {:s}\".format(str(type(zero_torch))))\n",
    "\n",
    "# Torch -> Numpy: simply call the numpy() method.\n",
    "zero_np = np.zeros((3, 2))\n",
    "assert (zero_torch.numpy() == zero_np).all()\n",
    "\n",
    "# Numpy -> Torch: simply call the corresponding function on the np.array.\n",
    "zero_torch_float = torch.FloatTensor(zero_np)\n",
    "print(\"\\nFloat:\\n\", zero_torch_float)\n",
    "zero_torch_int = torch.LongTensor(zero_np)\n",
    "print(\"Int:\\n\", zero_torch_int)\n",
    "zero_torch_bool = torch.BoolTensor(zero_np)\n",
    "print(\"Bool:\\n\", zero_torch_bool)\n",
    "\n",
    "# Reshape\n",
    "print(\"\\nView new shape...\", zero_torch.view(1, 6))\n",
    "# Note that print(zero_torch.reshape(1, 6)) would work too.\n",
    "# The difference is in how memory is handled (view imposes contiguity).\n",
    "\n",
    "# Algebra\n",
    "a = torch.randn((3, 2))\n",
    "b = torch.randn((3, 2))\n",
    "print(\"\\nAlgebraic operations are overloaded:\\n\", a, \"\\n+\\n\", b, \"\\n=\\n\", a + b)\n",
    "\n",
    "# More generally, torch shares the syntax of many attributes and functions with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor is a similar yet more complicated data structure than np.array.\n",
    "# It is basically a static array of number but may also contain an overlay to\n",
    "# handle automatic differentiation (i.e keeping track of the gradient and which\n",
    "# tensors depend on which).\n",
    "# To access the static array embedded in a tensor, simply call the detach() method\n",
    "print(zero_torch.detach())\n",
    "\n",
    "# When inside a function performing automatic differentiation (basically when training\n",
    "# a neural network), never use detach() otherwise meta information regarding gradients\n",
    "# will be lost, effectively freezing the variable and preventing backprop for it.\n",
    "# However when returning the result of training, do use detach() to save memory\n",
    "# (the naked tensor data uses much less memory than the full-blown tensor with gradient\n",
    "# management, and is much less prone to mistake such as bad copy and memory leak).\n",
    "\n",
    "# We will solve theta * x = y in theta for x=1 and y=2\n",
    "x = torch.ones(1)\n",
    "y = 2 * torch.ones(1)\n",
    "\n",
    "# Actually by default torch does not add the gradient management overlay\n",
    "# when declaring tensors like this. To force it, add requires_grad=True.\n",
    "theta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Optimisation routine\n",
    "# (Adam is a sophisticated variant of SGD, with adaptive step).\n",
    "optimizer = optim.Adam(params=[theta], lr=0.1)\n",
    "\n",
    "# Loss function\n",
    "print(\"Initial guess:\", theta.detach())\n",
    "\n",
    "for _ in range(100):\n",
    "    # By default, torch accumulates gradients in memory.\n",
    "    # To obtain the desired gradient descent beahviour,\n",
    "    # just clean the cached gradients using the following line:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Quadratic loss (* and ** are overloaded so that torch\n",
    "    # knows how to differentiate them)\n",
    "    loss = (y - theta * x) ** 2\n",
    "\n",
    "    # Apply the chain rule to automatically compute gradients\n",
    "    # for all relevant tensors.\n",
    "    loss.backward()\n",
    "\n",
    "    # Run one step of optimisation routine.\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final estimate:\", theta.detach())\n",
    "print(\"The final estimate should be close to\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, every neural network is built by subclassing `nn.Module`.\n",
    "# It defines:\n",
    "#   - how parameters are stored,\n",
    "#   - how theyâ€™re moved to GPU/CPU,\n",
    "#   - the forward pass computation\n",
    "# nn.Module is a way to organize layers and parameters\n",
    "\n",
    "### Layers\n",
    "\n",
    "linear_layer = nn.Linear(in_features=4, out_features=2)\n",
    "x = torch.randn(5, 4)  # batch of 3 samples, each with 4 features\n",
    "print(linear_layer(x))  # output will have shape (3, 2)\n",
    "\n",
    "\n",
    "### Custom architecture\n",
    "\n",
    "\n",
    "# Example: a simple two-layer network for classification\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()  # initialize the base nn.Module\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the function computed by the network\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNet(input_dim=4, hidden_dim=16, output_dim=2)\n",
    "print(model)\n",
    "\n",
    "# Access model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "# Run a forward pass\n",
    "x = torch.randn(5, 4)  # batch of 5 samples, each with 4 features\n",
    "y = model(x)\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A neural net task: Recognizing the speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = torchaudio.datasets.LIBRISPEECH(\".\", url=\"dev-clean\", download=True)\n",
    "print(f\"Number of data points : {len(ds)}\")\n",
    "\n",
    "# Example: access a data point\n",
    "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = ds[7]\n",
    "print(transcript)\n",
    "print(speaker_id)\n",
    "Audio(waveform.numpy()[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to check the number of unique speakers in the dataset\n",
    "\n",
    "speaker_ids = set()\n",
    "for _, _, _, speaker_id, _, _ in ds:\n",
    "    speaker_ids.add(speaker_id)\n",
    "\n",
    "print(f\"Dataset has {len(speaker_ids)} unique speakers in {len(ds)} points\")\n",
    "print(speaker_ids)\n",
    "## Dataset has 40 unique speakers in 2703 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look more in detail at some data points\n",
    "\n",
    "Instead of working on the pure waveform, we help our model and compute a spectrogram\n",
    "which we will feed as input to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Transcript: {transcript}\")\n",
    "print(f\"{waveform.shape=}\")\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(waveform.t().numpy())\n",
    "plt.title(\"Waveform\")\n",
    "plt.xlabel(\"Time (frames)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# compute mel spectrogram and display it\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=64\n",
    ").to(device)\n",
    "mel_db_transform = torchaudio.transforms.AmplitudeToDB().to(device)\n",
    "\n",
    "waveform.to(device)\n",
    "\n",
    "mel_spec = mel_transform(waveform)  # [1, n_mels, time]\n",
    "mel_db = mel_db_transform(mel_spec)  # in dB\n",
    "\n",
    "mel_db_np = mel_db.squeeze(0).numpy()  # [n_mels, time]\n",
    "\n",
    "\n",
    "# axes for display\n",
    "times = np.linspace(0, waveform.shape[1] / sample_rate, mel_db_np.shape[1])\n",
    "freqs = np.linspace(0, sample_rate / 2, mel_db_np.shape[0])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(\n",
    "    mel_db_np,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"magma\",\n",
    "    extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    ")\n",
    "print(mel_db_np.shape)\n",
    "plt.colorbar(label=\"dB\")\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idiomatic pytorch example of a Convolutional Neural Network\n",
    "\n",
    "nn.Sequential take a tuple of layer and applies them sequential in the order provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dMel(nn.Module):\n",
    "    def __init__(self, n_mels=64, n_classes=40):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_mels, 128, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 256, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),  # activations have shape that depend on the input time dimension\n",
    "            nn.AdaptiveAvgPool1d(\n",
    "                1\n",
    "            ),  # 'adaptive' means output size is fixed to 1 regardless of input size\n",
    "        )\n",
    "        self.fc = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, n_mels, time\n",
    "        x = self.conv(x)  # batch, n_classes, 1\n",
    "        return self.fc(x.squeeze(-1))\n",
    "\n",
    "\n",
    "model = Conv1dMel().to(device)\n",
    "\n",
    "# 1rst dimension is batch size\n",
    "print(mel_db.shape)  # [1, n_mels, time]\n",
    "print(model(mel_db.to(device)).shape)  # [1, n_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemement your own Convolutional Layer\n",
    "In this first section, we will reimplement the first layer ourselves, step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv1in1out(nn.Module):\n",
    "    # kernel_size: size of the convolution kernel\n",
    "    # stride: how many steps to move the kernel at each step\n",
    "    # padding: how many zeros to add at the begining and the end of the input\n",
    "\n",
    "    def __init__(self, kernel_size, stride=1, padding=2):\n",
    "        super().__init__()\n",
    "        # self.kernel = nn.Parameter(torch.randn(???) * 0.01)\n",
    "        # YOUR CODE HERE\n",
    "        # self.bias = nn.Parameter(???)\n",
    "        # YOUR CODE HERE\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, time\n",
    "        x = F.pad(x, (self.padding, self.padding))\n",
    "        batch, length = x.shape\n",
    "        k = self.kernel.shape[-1]\n",
    "        out_length = (length - k) // self.stride + 1\n",
    "\n",
    "        # Create output tensor\n",
    "        # out = torch.zeros(????, device=x.device)\n",
    "        out = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Check your implementation\n",
    "custom_conv = CustomConv1in1out(5).to(device)\n",
    "\n",
    "x = mel_db[:, 0, :]  # take first mel channel\n",
    "# 1rst dimension is batch size\n",
    "print(x.shape)  # [1, n_mels, time]\n",
    "print(custom_conv(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv1out(nn.Module):\n",
    "    # kernel_size: size of the convolution kernel\n",
    "    # stride: how many steps to move the kernel at each step\n",
    "    # padding: how many zeros to add at the begining and the end of the input\n",
    "\n",
    "    def __init__(self, kernel_size, in_channels, stride=1, padding=2):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        out = None\n",
    "        return out\n",
    "\n",
    "\n",
    "custom_conv = CustomConv1out(5, 64).to(device)\n",
    "\n",
    "x = mel_db\n",
    "print(x.shape)  # [1, n_mels, time]\n",
    "print(custom_conv(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv(nn.Module):\n",
    "    # kernel_size: size of the convolution kernel\n",
    "    # stride: how many steps to move the kernel at each step\n",
    "    # padding: how many zeros to add at the begining and the end of the input\n",
    "\n",
    "    def __init__(self, kernel_size, in_channels, out_channels, stride=1, padding=2):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, in_channels, time\n",
    "        # YOUR CODE HERE\n",
    "        out = None\n",
    "        return out\n",
    "\n",
    "\n",
    "custom_conv = CustomConv(5, 64, 128).to(device)\n",
    "\n",
    "x = mel_db  # take first mel channel\n",
    "# 1rst dimension is batch size\n",
    "print(f\"{x.shape=}\")  # [1, n_mels, time]\n",
    "print(f\"Output shape {custom_conv(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "The for loop breaks parallelization possibilities. Instead, we reformulate \n",
    "the computations as tensor operations with an extra dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvVectorized(nn.Module):\n",
    "    # kernel_size: size of the convolution kernel\n",
    "    # stride: how many steps to move the kernel at each step\n",
    "    # padding: how many zeros to add at the begining and the end of the input\n",
    "\n",
    "    def __init__(self, kernel_size, in_channels, out_channels, stride=1, padding=2):\n",
    "        super().__init__()\n",
    "        self.kernel = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, kernel_size) * 0.01\n",
    "        )\n",
    "        # kernel is one matrix per time step in window\n",
    "        self.bias = nn.Parameter(torch.zeros(1, out_channels, 1))\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, in_channels, time\n",
    "        x = F.pad(x, (self.padding, self.padding))\n",
    "        b, in_ch, length = x.shape\n",
    "        _, out_ch, _, k = self.kernel.shape\n",
    "        k = self.kernel.shape[-1]\n",
    "\n",
    "        out_length = (length - k) // self.stride + 1\n",
    "        stride = self.stride\n",
    "\n",
    "        idx = torch.arange(k, device=x.device) + torch.arange(\n",
    "            0, out_length * stride, stride, device=x.device\n",
    "        ).unsqueeze(1)\n",
    "        # idx[i, j] = start position of window i + offset j\n",
    "\n",
    "        # Gather all windows at once: x[:, :, idx] -> (batch, in_ch, out_length, k)\n",
    "        x_slices = x[:, :, idx]  # broadcasting gather\n",
    "\n",
    "        # Reshape kernel to (1, out_ch, in_ch, 1, k)\n",
    "        kernel = self.kernel.view(1, out_ch, in_ch, 1, k)\n",
    "        x_slices = x_slices.unsqueeze(1)  # (b, 1, in_ch, out_length, k)\n",
    "\n",
    "        # kernel * x_slices: (b, out_ch, in_ch, out_length, k)\n",
    "\n",
    "        # Elementwise multiply and sum over in_ch and k\n",
    "        out = (x_slices * kernel).sum(\n",
    "            dim=(2, 4)\n",
    "        ) + self.bias  # (batch, out_ch, out_length)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Check that our implementations give the same result\n",
    "\n",
    "custom_conv_layer = CustomConv(5, 64, 128).to(device)\n",
    "custom_conv_vectorized = CustomConvVectorized(5, 64, 128).to(device)\n",
    "model_pytorch = nn.Conv1d(64, 128, 5, stride=1, padding=2).to(device)\n",
    "\n",
    "# Safe way to copy the weights from model to model_vec, without affecting the flow\n",
    "# of gradients in the computation graph\n",
    "with torch.no_grad():\n",
    "    custom_conv_vectorized.kernel.data.copy_(custom_conv_layer.kernel.data)\n",
    "    custom_conv_vectorized.bias.data.copy_(custom_conv_layer.bias.data.unsqueeze(-1))\n",
    "\n",
    "    model_pytorch.weight.copy_(custom_conv_layer.kernel.squeeze(0))\n",
    "    model_pytorch.bias.copy_(custom_conv_layer.bias.squeeze(0))\n",
    "\n",
    "x = mel_db.to(device)  # take first mel channel\n",
    "# 1rst dimension is batch size\n",
    "print(x.shape)  # [1, n_mels, time]\n",
    "print(custom_conv_vectorized(x).shape)\n",
    "print(model_pytorch(x).shape)\n",
    "\n",
    "print(custom_conv_layer(x))\n",
    "print(custom_conv_vectorized(x))\n",
    "print(model_pytorch(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit model(x)\n",
    "%timeit model_vec(x)\n",
    "%timeit model_pytorch(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_pytorch(x).squeeze(0).detach().numpy()\n",
    "\n",
    "plt.imshow(\n",
    "    output.T,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"magma\",\n",
    "    extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMaxPoolNaive(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, in_channels, time\n",
    "        b, in_ch, length = x.shape\n",
    "        k = self.kernel_size\n",
    "        out_length = (length - k) // self.stride + 1\n",
    "\n",
    "        # Create output tensor\n",
    "        out = None\n",
    "        # YOUR CODE HERE\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomMaxPoolVec(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, in_channels, time\n",
    "        _, in_ch, length = x.shape\n",
    "        k = self.kernel_size\n",
    "        out_length = (length - k) // self.stride + 1\n",
    "\n",
    "        # Build the indices of each pooling window\n",
    "        idx = torch.arange(k, device=x.device)  # (K,)\n",
    "        start = torch.arange(0, out_length * self.stride, self.stride, device=x.device)[\n",
    "            :, None\n",
    "        ]  # (L_out, 1)\n",
    "        indices = start + idx  # shape (L_out, K)\n",
    "        # e.g., [[0,1,2], [2,3,4], [4,5,6], ...]\n",
    "\n",
    "        # x: (B, C, L) ->  (B, C, L_out, K)\n",
    "        x_expanded = x[:, :, indices]  # x_expanded[b, c, i, k] = x[b, c, indices[i, k]]\n",
    "\n",
    "        # Now take the max over the window dimension\n",
    "        return x_expanded.max(dim=-1).values  # (B, C, L_out)\n",
    "\n",
    "\n",
    "max_pool = CustomMaxPoolNaive(2, stride=1).to(device)\n",
    "max_pool_vec = CustomMaxPoolNaive(2, stride=1).to(device)\n",
    "max_pool_pytorch = nn.MaxPool1d(2, stride=1).to(device)\n",
    "\n",
    "x = mel_db.to(device)\n",
    "print(max_pool(x) - max_pool_pytorch(x))\n",
    "print(max_pool_vec(x) - max_pool_pytorch(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit max_pool(x)\n",
    "%timeit max_pool_vec(x)\n",
    "%timeit max_pool_pytorch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1dMel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset loader\n",
    "Create batches to feed the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mapping from speaker id to class index (use existing speaker_list if available)\n",
    "spk2idx = {spk: i for i, spk in enumerate(speaker_ids)}\n",
    "\n",
    "\n",
    "def get_batches(indices, batch_size):\n",
    "    return [indices[i : i + batch_size] for i in range(0, len(indices), batch_size)]\n",
    "\n",
    "\n",
    "# Padding: since different input sequences have different lenghths, we need to pad them\n",
    "# add a default value at the start or the end) to the same length in a batch\n",
    "def fetch_batch(batch_idxs, ds, device=device):\n",
    "    waves = []\n",
    "    targets = []\n",
    "    for idx in batch_idxs:\n",
    "        waveform, _, _, speaker_id, _, _ = ds[idx]\n",
    "        waves.append(waveform)  # waveform shape: [1, time]\n",
    "        targets.append(spk2idx[speaker_id])\n",
    "\n",
    "    # pad to the longest waveform in the batch\n",
    "    max_len = max(w.shape[1] for w in waves)\n",
    "    dtype = waves[0].dtype\n",
    "    batch_wave = torch.zeros(len(waves), 1, max_len, dtype=dtype, device=device)\n",
    "    for i, w in enumerate(waves):\n",
    "        batch_wave[i, :, : w.shape[1]] = w\n",
    "\n",
    "    return batch_wave, torch.tensor(targets, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function on test set\n",
    "\n",
    "# Convert the output of the model into predictions\n",
    "\n",
    "\n",
    "def evaluate(model, ds, test_indices, device, batch_size=8):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        eval_batches = get_batches(test_indices, batch_size)\n",
    "        for batch_idxs in eval_batches:\n",
    "            batch_wave, targets = fetch_batch(batch_idxs, ds, device=device)\n",
    "\n",
    "            mel = mel_transform(batch_wave)\n",
    "            mel_db = mel_db_transform(mel).squeeze(1)  # batch, n_mels, time\n",
    "            # YOUR CODE HERE\n",
    "            preds = None\n",
    "\n",
    "            correct += int((preds == targets).sum().item())\n",
    "\n",
    "    accuracy = correct / len(test_indices)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "- shuffle the dataset, then separate train-test.\n",
    "- traverse the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(range(0, len(ds)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "N_train = int(0.8 * len(ds))\n",
    "train_indices = indices[:N_train]\n",
    "test_indices = indices[N_train:]\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.train()\n",
    "loss_history = []\n",
    "test_acc = []\n",
    "test_times = []\n",
    "for e in range(epochs):\n",
    "    train_batches = get_batches(train_indices, batch_size)\n",
    "    running_loss = 0.0\n",
    "    for step, batch_idxs in enumerate(train_batches):\n",
    "        batch_wave, targets = fetch_batch(batch_idxs, ds, device=device)\n",
    "        mel = mel_transform(batch_wave)\n",
    "        mel_db = mel_db_transform(mel).squeeze(1)  # batch, n_mels, time\n",
    "\n",
    "        # Forward pass\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Backward pass\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            test_times.append(step)\n",
    "            avg = running_loss / 100\n",
    "            print(\n",
    "                f\"Epoch {e + 1}/{epochs}  Step {step}/{len(train_batches)} \"\n",
    "                + f\"AvgLoss {avg:.4f}\"\n",
    "            )\n",
    "            test_acc.append(evaluate(model, ds, test_indices, device, batch_size))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch = e + 1  # update epoch variable for later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Train loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "plt.title(\"Test accuracy\")\n",
    "plt.plot(test_times, test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first conv layer weights from available candidates and plot them.\n",
    "# This cell reuses imports/variables already present in the notebook (torch, np, plt, nn).\n",
    "\n",
    "# find a conv1d weight tensor (prefer model.conv[0] if available)\n",
    "conv_weight = None\n",
    "\n",
    "conv_weight = model.conv[0].weight.detach().cpu().numpy()  # (out_ch, in_ch, kernel)\n",
    "\n",
    "# conv_weight shape: (out_ch, in_ch, kernel_size)\n",
    "out_ch, in_ch, k = conv_weight.shape\n",
    "print(\n",
    "    f\"Found conv weights with shape (out_channels, in_channels, kernel_size): {conv_weight.shape}\"\n",
    ")\n",
    "\n",
    "# plotting: show each output filter as a small heatmap of shape (in_ch, kernel_size)\n",
    "ncols = min(8, out_ch)\n",
    "nrows = int(np.ceil(out_ch / ncols))\n",
    "\n",
    "maxabs = np.max(np.abs(conv_weight)) if conv_weight.size else 1.0\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 1.6), squeeze=False)\n",
    "for i in range(nrows * ncols):\n",
    "    r = i // ncols\n",
    "    c = i % ncols\n",
    "    ax = axes[r][c]\n",
    "    if i < out_ch:\n",
    "        im = ax.imshow(\n",
    "            conv_weight[i],\n",
    "            origin=\"lower\",\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"bwr\",\n",
    "            vmin=-maxabs,\n",
    "            vmax=maxabs,\n",
    "        )\n",
    "        ax.set_title(f\"filter {i}\")\n",
    "        ax.set_xlabel(\"kernel pos\")\n",
    "        ax.set_ylabel(\"in_ch\")\n",
    "    else:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "# shared colorbar\n",
    "# fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "fig.suptitle(\n",
    "    \"First Conv1D Layer Filters (each heatmap = in_channels x kernel_size)\", fontsize=14\n",
    ")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    conv_weight[79],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"bwr\",\n",
    "    vmin=-maxabs,\n",
    "    vmax=maxabs,\n",
    ")\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
