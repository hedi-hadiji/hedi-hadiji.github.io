# Theoretical Principles of Deep Learning 

CentraleSupélec - M2 Maths-IA Université Paris-Saclay

**Instructor**: [Hedi Hadiji](https://hedi-hadiji.github.io/) 

Reach me at: hedi.hadiji@l2s.centralesupelec.fr

### Course Description

Machine learning algorithms involving deep neural networks have accumulated spectacular empirical successes over the recent years. Many of those accomplishments cannot be explained by conventional wisdom coming from standard learning theory. As the popularity of deep learning grows, the gap between theory and practice keeps widening. Building the groundworks for a satisfying theory of deep learning, with the ultimate goal of providing valuable insights to practitioners, is a major challenge in modern research.

In this class we will discuss recent theoretical progress made towards describing the empirical performance of deep methods. Our main focus will be the study of the surprisingly good *generalization* ability of deep networks. 

## Slides

<p>
    <strong>Class 1: Introduction and Core Concepts</strong>.
    Approximation, Optimization and Generalization. Single-Layer Perceptron. 
    [<a href=tdl_slides/Lect1.pdf>Slides</a>]
    </br>
</p>

<p>
    <strong>Class 2: Approximation with Neural nets</strong>.
    Universality of approximation. Barron's theorem. 
    [<a href=tdl_slides/Lect2.pdf>Slides</a>]
    </br>
</p>

<p>
    <strong>Class 3: Lazy Training</strong>.
    Optimization in the lazy regime. Insights from overparameterized linear models. 
    [<a href=tdl_slides/Lect3.pdf>Slides</a>]
    </br>
</p>

<p>
    <strong>Class 4: Generalization</strong>.
    Concentration. Rademacher complexity.  
    [<a href=tdl_slides/Lect4.pdf>Slides</a>]
    </br>
</p>

<p>
    <strong>Class 5: The Neural Tangent Kernel</strong>.
    Definition and computation of Neural Tangent Kernels. Generalization
    bounds for the NTK.  
    [<a href=tdl_slides/Lect5.pdf>Slides</a>]
    </br>
</p>

<p>
    <strong>Class 6: PAC-Bayes Generalization bounds</strong>.
    PAC-Bayes bounds. MacAllester's bound. Non-vacuous bounds for stochastic
     networks.
    [<a href=tdl_slides/Lect6.pdf>Slides</a>]
    </br>
</p>

<p>
    <strong>Class 7: Some mysteries</strong>.
    The Edge of stability. Feature learning. 
    </br>
</p>

<!-- ```{tableofcontents}
``` -->
